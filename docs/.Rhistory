knitr::opts_chunk$set(warning = FALSE, message = FALSE)
knitr::opts_knit$set(root.dir = "~/Dropbox/text-as-data-JUST-CORY-AND-AMBER/modules_2024/data/")
trump_tweets_corp = readRDS("trump_tweet_corp.rds")
knitr::opts_knit$set(root.dir = "~/Dropbox/text-as-data-JUST-CORY-AND-AMBER/modules_2024/data/")
# Load packages
require(tidyverse)
require(quanteda)
require(quanteda.textstats)
require(quanteda.textplots)
library(stringr)
library(ggplot2)
library(ggpubr)
# Set working directory
setwd("~/Dropbox/text-as-data-JUST-CORY-AND-AMBER/modules_2024/data/")
getwd() # view working directory
trump_tweets_corp = readRDS("trump_tweet_corp.rds")
head(docvars(trump_tweets_corp),10)
# Transform to tokens
trump_tweet_toks = trump_tweets_corp  %>%
tokens(remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE,
remove_url = TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords("english"))
# Add collocations
trump_tweet_coll = textstat_collocations(trump_tweet_toks, size = 2:3, min_count = 10)
trump_tweet_coll
# Add to tokens document
trump_toks_all =  tokens_compound(trump_tweet_toks, trump_tweet_coll, concatenator = " ") %>%
tokens_select(padding=FALSE) %>% # remove padding
tokens_wordstem # stem words after adding collocations
tail(trump_toks_all)
<center>![](~/Dropbox/text-as-data-JUST-CORY-AND-AMBER/modules/images/bag of words.png){width="70%"}</center>
# Create a document-feature matrix
trump_tweet_dfm = dfm(trump_toks_all)
trump_tweet_dfm
# Number of features (i.e., unique tokens)
nfeat(trump_tweet_dfm)
# Examine sums across documents
head(colSums(trump_tweet_dfm),50)
# Number of features (i.e., unique tokens)
nfeat(trump_tweet_dfm)
# Pipe grouping through trimming step
trump_tweet_dfm_trim = dfm(trump_tweet_dfm) %>%
dfm_trim(min_termfreq = 10, min_docfreq = 10, docfreq_type = "count") %>%
dfm_group(groups = source)
print(trump_tweet_dfm_trim)
# Get frequency and ranking
textstat_frequency(trump_tweet_dfm_trim)
set.seed(132) # set seed so figure can be reproduced
textplot_wordcloud(tweet_dfm_trim_source, comparison = TRUE, max_words = 200, color = c("coral","dodgerblue"), min_size=0.9)
textplot_wordcloud(tweet_dfm_trim, comparison = TRUE, max_words = 200, color = c("coral","dodgerblue"), min_size=0.9)
textplot_wordcloud(trump_tweet_dfm_trim, comparison = TRUE, max_words = 200, color = c("coral","dodgerblue"), min_size=0.9)
tweet_words_freq = dfm(tweet_dfm_trim) %>%
textstat_frequency(groups = source) # grouping the term frequencies by source
trump_tweet_words_freq = dfm(trump_tweet_dfm_trim) %>%
textstat_frequency(groups = source) # grouping the term frequencies by source
head(trump_tweet_words_freq)
# First create dfs with top 15 features
android15_df = trump_tweet_words_freq %>%
filter(group == "Android") %>%
head(15)
iphone15_df = trump_tweet_words_freq %>%
filter(group == "iPhone") %>%
head(15)
# Plot and arrange side by side
android15_plot = ggplot(data = android15_df, aes(x = reorder(feature, frequency), y = frequency)) +
geom_col(stat = "identity", alpha = 0.8, fill = "coral",
position = position_dodge(width = 0.8)) +
scale_y_continuous(limits = c(0, 200)) +
xlab("Frequency") + ylab("") +
ggtitle("Android") +
coord_flip() +
theme_bw()
iphone15_plot = ggplot(data = iphone15_df, aes(x = reorder(feature, frequency), y = frequency)) +
geom_col(stat = "identity", alpha = 0.8, fill = "dodgerblue",
position = position_dodge(width = 0.8)) +
scale_y_continuous(limits = c(0, 200)) +
xlab("Frequency") + ylab("") +
ggtitle("iPhone") +
coord_flip() +
theme_bw()
ggpubr::ggarrange(android15_plot, iphone15_plot) %>%
annotate_figure(top = text_grob("Top terms in Trump's Android and iPhone Tweets",  size = 14))
# First create dfs with top 15 features
android15_df = trump_tweet_words_freq %>%
filter(group == "Android") %>%
head(20)
iphone15_df = trump_tweet_words_freq %>%
filter(group == "iPhone") %>%
head(20)
# Plot and arrange side by side
android15_plot = ggplot(data = android15_df, aes(x = reorder(feature, frequency), y = frequency)) +
geom_col(stat = "identity", alpha = 0.8, fill = "coral",
position = position_dodge(width = 0.8)) +
scale_y_continuous(limits = c(0, 200)) +
xlab("Frequency") + ylab("") +
ggtitle("Android") +
coord_flip() +
theme_bw()
iphone15_plot = ggplot(data = iphone15_df, aes(x = reorder(feature, frequency), y = frequency)) +
geom_col(stat = "identity", alpha = 0.8, fill = "dodgerblue",
position = position_dodge(width = 0.8)) +
scale_y_continuous(limits = c(0, 200)) +
xlab("Frequency") + ylab("") +
ggtitle("iPhone") +
coord_flip() +
theme_bw()
ggpubr::ggarrange(android15_plot, iphone15_plot) %>%
annotate_figure(top = text_grob("Top terms in Trump's Android and iPhone Tweets",  size = 14))
# First create dfs with top 15 features
android15_df = trump_tweet_words_freq %>%
filter(group == "Android") %>%
head(25)
iphone15_df = trump_tweet_words_freq %>%
filter(group == "iPhone") %>%
head(25)
# Plot and arrange side by side
android15_plot = ggplot(data = android15_df, aes(x = reorder(feature, frequency), y = frequency)) +
geom_col(stat = "identity", alpha = 0.8, fill = "coral",
position = position_dodge(width = 0.8)) +
scale_y_continuous(limits = c(0, 200)) +
xlab("Frequency") + ylab("") +
ggtitle("Android") +
coord_flip() +
theme_bw()
iphone15_plot = ggplot(data = iphone15_df, aes(x = reorder(feature, frequency), y = frequency)) +
geom_col(stat = "identity", alpha = 0.8, fill = "dodgerblue",
position = position_dodge(width = 0.8)) +
scale_y_continuous(limits = c(0, 200)) +
xlab("Frequency") + ylab("") +
ggtitle("iPhone") +
coord_flip() +
theme_bw()
ggpubr::ggarrange(android15_plot, iphone15_plot) %>%
annotate_figure(top = text_grob("Top terms in Trump's Android and iPhone Tweets",  size = 14))
# First create dfs with top 15 features
android15_df = trump_tweet_words_freq %>%
filter(group == "Android") %>%
head(15)
iphone15_df = trump_tweet_words_freq %>%
filter(group == "iPhone") %>%
head(15)
# Plot and arrange side by side
android15_plot = ggplot(data = android15_df, aes(x = reorder(feature, frequency), y = frequency)) +
geom_col(stat = "identity", alpha = 0.8, fill = "coral",
position = position_dodge(width = 0.8)) +
scale_y_continuous(limits = c(0, 200)) +
xlab("Frequency") + ylab("") +
ggtitle("Android") +
coord_flip() +
theme_bw()
iphone15_plot = ggplot(data = iphone15_df, aes(x = reorder(feature, frequency), y = frequency)) +
geom_col(stat = "identity", alpha = 0.8, fill = "dodgerblue",
position = position_dodge(width = 0.8)) +
scale_y_continuous(limits = c(0, 200)) +
xlab("Frequency") + ylab("") +
ggtitle("iPhone") +
coord_flip() +
theme_bw()
ggpubr::ggarrange(android15_plot, iphone15_plot) %>%
annotate_figure(top = text_grob("Top terms in Trump's Android and iPhone Tweets",  size = 14))
textplot_wordcloud(trump_tweet_dfm_trim, comparison = TRUE, max_words = 250, color = c("coral","dodgerblue"), min_size=0.9)
550*4
2200*.10
knitr::opts_knit$set(root.dir = "~/Dropbox/text-as-data-JUST-CORY-AND-AMBER/modules_2024/data")
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
knitr::opts_knit$set(root.dir = "~/Dropbox/text-as-data-JUST-CORY-AND-AMBER/modules_2024/data")
require(tidyverse)
require(quanteda)
require(quanteda.textstats)
require(quanteda.textplots)
library(stringr)
library(ggplot2)
library(ggpubr)
# Set working directory
setwd("~/Dropbox/text-as-data-JUST-CORY-AND-AMBER/modules_2024/data/")
getwd() # view working directory
Read in corpus
# Create a document-feature matrix
trump_tweet_dfm = dfm(trump_toks_all)
trump_tweet_dfm
# Number of features (i.e., unique tokens)
nfeat(trump_tweet_dfm)
# Read in corpus
trump_tweets_corp = readRDS("trump_tweet_corp.rds")
head(docvars(trump_tweets_corp),10)
# Transform to tokens
trump_tweet_toks = trump_tweets_corp  %>%
tokens(remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE,
remove_url = TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords("english"))
# Add collocations
trump_tweet_coll = textstat_collocations(trump_tweet_toks, size = 2:3, min_count = 10)
trump_tweet_coll
# Add to tokens document
trump_toks_all =  tokens_compound(trump_tweet_toks, trump_tweet_coll, concatenator = " ") %>%
tokens_select(padding=FALSE) %>% # remove padding
tokens_wordstem # stem words after adding collocations
tail(trump_toks_all)
# Create a document-feature matrix
trump_tweet_dfm = dfm(trump_toks_all)
trump_tweet_dfm
# Number of features (i.e., unique tokens)
nfeat(trump_tweet_dfm)
# Examine sums across documents
head(colSums(trump_tweet_dfm),50)
# Pipe grouping through trimming step
trump_tweet_dfm_trim = dfm(trump_tweet_dfm) %>%
dfm_trim(min_termfreq = 10, min_docfreq = 10, docfreq_type = "count") %>%
dfm_group(groups = source)
print(trump_tweet_dfm_trim)
# Get frequency and ranking
textstat_frequency(trump_tweet_dfm_trim)
# Get frequency and ranking
x = textstat_frequency(trump_tweet_dfm_trim)
View(x)
print(trump_tweet_dfm_trim)
View(x)
x = trump_tweet_dfm_trim
View(x)
trump_tweet_words_freq = dfm(trump_tweet_dfm_trim) %>%
textstat_frequency(groups = source) # Grouping the term frequencies by source
head(trump_tweet_words_freq)
x = trump_tweet_words_freq
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
knitr::opts_knit$set(root.dir = "~/Dropbox/text-as-data-JUST-CORY-AND-AMBER/modules_2024/data")
require(tidyverse)
require(quanteda)
require(quanteda.textstats)
require(quanteda.textplots)
library(stringr)
library(ggplot2)
library(ggpubr)
# Set working directory
setwd("~/Dropbox/text-as-data-JUST-CORY-AND-AMBER/modules_2024/data/")
getwd() # view working directory
# Read in corpus
trump_tweets_corp = readRDS("trump_tweet_corp.rds")
head(docvars(trump_tweets_corp),10)
# Transform to tokens
trump_tweet_toks = trump_tweets_corp  %>%
tokens(remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE,
remove_url = TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords("english"), padding=TRUE)
# Add collocations
trump_tweet_coll = textstat_collocations(trump_tweet_toks, size = 2:3, min_count = 10)
trump_tweet_coll
# Add to tokens document
trump_toks_all =  tokens_compound(trump_tweet_toks, trump_tweet_coll, concatenator = " ") %>%
tokens_select(padding=FALSE) %>% # remove padding
tokens_wordstem # stem words after adding collocations
tail(trump_toks_all)
trump_tweet_toks
# Transform to tokens
trump_tweet_toks = trump_tweets_corp  %>%
tokens(remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE,
remove_url = TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords("english"), padding=TRUE)
trump_tweet_toks
# Add collocations
trump_tweet_coll = textstat_collocations(trump_tweet_toks, size = 2:3, min_count = 10)
trump_tweet_coll
# Add to tokens document
trump_toks_all =  tokens_compound(trump_tweet_toks, trump_tweet_coll, concatenator = " ") %>%
tokens_select(padding=FALSE) %>% # remove padding
tokens_wordstem # stem words after adding collocations
tail(trump_toks_all)
head(trump_toks_all)
tail(trump_toks_all)
# Add to tokens document
trump_toks_all =  tokens_compound(trump_tweet_coll, trump_tweet_toks, concatenator = " ") %>%
tokens_select(padding=FALSE) %>% # remove padding
tokens_wordstem # stem words after adding collocations
# Transform to tokens
trump_tweet_toks = trump_tweets_corp  %>%
tokens(remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE,
remove_url = TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords("english"), padding=TRUE)
# Add collocations
trump_tweet_coll = textstat_collocations(trump_tweet_toks, size = 2:3, min_count = 10)
# Add to tokens document
trump_toks_all =  tokens_compound(trump_tweet_coll, trump_tweet_toks, concatenator = " ") %>%
tokens_select(padding=FALSE) %>% # remove padding
tokens_wordstem # stem words after adding collocations
trump_tweet_coll
# Add to tokens document
trump_toks_all =  tokens_compound(trump_tweet_toks, trump_tweet_coll, concatenator = " ") %>%
tokens_select(padding=FALSE) %>% # remove padding
tokens_wordstem # stem words after adding collocations
fuck = trump_toks_all
View(fuck)
View(trump_tweet_coll)
wtf  = trump_tweet_toks
View(wtf)
# Find collocations in tokens document
trump_toks_all =  tokens_compound(trump_tweet_toks, trump_tweet_coll, concatenator = " ") %>%
tokens_select(padding=FALSE) %>% # remove padding
tokens_wordstem # stem words after adding collocations
dongetit = trump_toks_all
View(dongetit)
tail(trump_toks_all)
tail(trump_toks_all,15)
# Find collocations in tokens document
trump_toks_all =  tokens_compound(trump_tweet_toks, trump_tweet_coll, concatenator = " ", join=FALSE) %>%
tokens_select(padding=FALSE) %>% # remove padding
tokens_wordstem # stem words after adding collocations
tail(trump_toks_all,15)
# Find collocations in tokens document
trump_toks_all =  tokens_compound(trump_tweet_toks, trump_tweet_coll, concatenator = " ", join=TRUE) %>%
tokens_select(padding=FALSE) %>% # remove padding
tokens_wordstem # stem words after adding collocations
tail(trump_toks_all,15)
